{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# FULL PROJECT ANALYSIS: DATA PREPARATION, EDA, AND GARCH STRATEGY BACKTEST\n",
        "#\n",
        "# This script performs the following tasks:\n",
        "# 1. Loads raw tick data files and combines them (Data Preparation).\n",
        "# 2. Filters the combined data to the project period and calculates mid-price.\n",
        "# 3. Executes Exploratory Data Analysis (EDA) on the tick data.\n",
        "# 4. Implements the GARCH-Adaptive Volatility Breakout Strategy (Lecture 3).\n",
        "# 5. Calculates Net P&L, Sharpe Ratio, and Max Drawdown (Lecture 2).\n",
        "# ====================================================================\n",
        "\n",
        "# --- DEPENDENCIES ---\n",
        "!pip install arch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from arch import arch_model\n",
        "from datetime import datetime, timezone\n",
        "import os\n",
        "\n",
        "# --- GLOBAL CONFIGURATION ---\n",
        "# Data Input Paths (from Google Colab environment)\n",
        "FILE_PATH_SEP = '/content/EURPLN-2025-09.csv'\n",
        "FILE_PATH_OCT = '/content/EURPLN-2025-10.csv'\n",
        "\n",
        "# Project Timeline\n",
        "START_PERIOD = datetime(2025, 9, 1, 0, 0, 0, tzinfo=timezone.utc) # Start of full dataset\n",
        "END_PERIOD = datetime(2025, 10, 6, 0, 0, 0, tzinfo=timezone.utc)   # End of full dataset\n",
        "\n",
        "# Strategy Split Point (Start of Out-of-Sample/Testing Period)\n",
        "SPLIT_DATE = datetime(2025, 9, 22, 0, 0, 0, tzinfo=timezone.utc)\n",
        "\n",
        "# Output filename for the combined, cleaned data (for user validation)\n",
        "CLEAN_OUTPUT_FILE = '/content/EURPLN_01Sep_06Oct_clean_tick_data.csv'\n",
        "\n",
        "# Strategy Parameters\n",
        "K_FACTOR = 3.0       # Volatility scaling factor for breakout threshold\n",
        "COST_BPS = 0.0001    # Estimated transaction cost (1 basis point or 1 pip spread)\n",
        "# ====================================================================\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# SECTION 1: DATA PREPARATION AND EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# ====================================================================\n",
        "\n",
        "def load_and_combine_data():\n",
        "    \"\"\"Loads, combines, cleans, and exports the final tick data.\"\"\"\n",
        "    print(\"--- 1.1 Starting Data Loading and Combination ---\")\n",
        "\n",
        "    def load_file(filepath):\n",
        "        # Reads the CSV, assuming a standard tick format: [Time, Bid, Ask, ...]\n",
        "        try:\n",
        "            # === ROBUST LOADING ASSUMPTION (LIKELY FIX) ===\n",
        "            # Assume no header row (header=None) and that the timestamp is the first column (index_col=0).\n",
        "            # The columns are named generically and we select the Bid (V1) and Ask (V2) columns.\n",
        "            print(f\"Loading {os.path.basename(filepath)} assuming no header (Time, Bid, Ask, ...)\")\n",
        "            df = pd.read_csv(\n",
        "                filepath,\n",
        "                index_col=0,\n",
        "                parse_dates=True,\n",
        "                header=None, # Assume NO HEADER\n",
        "                names=['Time', 'Bid', 'Ask', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8'] # Assign generic names\n",
        "            )\n",
        "            df.index.name = 'Time'\n",
        "\n",
        "            # Select only the critical Bid and Ask columns\n",
        "            df = df[['Bid', 'Ask']].copy()\n",
        "\n",
        "            print(f\"Loaded {len(df)} ticks from {os.path.basename(filepath)}\")\n",
        "\n",
        "            # Print column types and first few rows for debugging if issues persist\n",
        "            # print(f\"Sample data head:\\n{df.head()}\")\n",
        "            # print(f\"Index type: {df.index.dtype}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            # Print detailed error to help diagnose issues if the format is truly non-standard\n",
        "            print(f\"CRITICAL ERROR loading {filepath}: {e}\")\n",
        "            print(\"--- DATA LOADING FAILED ---\")\n",
        "            print(\"If the error persists, please check your CSV structure:\")\n",
        "            print(\"1. Is the timestamp in the FIRST column?\")\n",
        "            print(\"2. Are the Bid and Ask prices in the SECOND and THIRD columns, respectively?\")\n",
        "            print(\"3. Is the date format consistent (e.g., YYYY-MM-DD HH:MM:SS.mmm)?\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    # Load and combine both files\n",
        "    df_sep = load_file(FILE_PATH_SEP)\n",
        "    df_oct = load_file(FILE_PATH_OCT)\n",
        "\n",
        "    if df_sep.empty or df_oct.empty:\n",
        "        # Re-raise error if data frames are still empty after loading\n",
        "        raise ValueError(\"One or both data files failed to load. Check file paths and column structure.\")\n",
        "\n",
        "    combined_df = pd.concat([df_sep, df_oct]).sort_index()\n",
        "    print(f\"Total combined ticks before filtering: {len(combined_df)}\")\n",
        "\n",
        "    # Calculate Mid Price\n",
        "    combined_df['Price'] = (combined_df['Bid'] + combined_df['Ask']) / 2\n",
        "\n",
        "    # Filter to exact period and clean\n",
        "    if combined_df.index.tz is None:\n",
        "        combined_df = combined_df.tz_localize(timezone.utc)\n",
        "\n",
        "    filtered_df = combined_df.loc[\n",
        "        (combined_df.index >= START_PERIOD) &\n",
        "        (combined_df.index < END_PERIOD)\n",
        "    ].copy()\n",
        "\n",
        "    filtered_df = filtered_df[~filtered_df.index.duplicated(keep='first')]\n",
        "    filtered_df.dropna(subset=['Price'], inplace=True)\n",
        "\n",
        "    print(f\"Final Ticks for Project ({START_PERIOD.strftime('%Y-%m-%d %H:%M:%S')} to {END_PERIOD.strftime('%Y-%m-%d %H:%M:%S')}): {len(filtered_df)}\")\n",
        "\n",
        "    # Export for Validation\n",
        "    filtered_df[['Price', 'Bid', 'Ask']].to_csv(CLEAN_OUTPUT_FILE)\n",
        "    print(f\"\\nSUCCESS: Cleaned data exported to: {CLEAN_OUTPUT_FILE} (for user review)\")\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "def run_eda(data):\n",
        "    \"\"\"Performs basic Exploratory Data Analysis on the prepared tick data.\"\"\"\n",
        "    print(\"\\n--- 1.2 Exploratory Data Analysis (EDA) ---\")\n",
        "\n",
        "    # Summary Statistics\n",
        "    print(\"\\nSummary Statistics of Tick Prices:\")\n",
        "    print(data['Price'].describe())\n",
        "\n",
        "    # Calculate Spread (Ask - Bid) in basis points (BPS)\n",
        "    data['Spread_BPS'] = (data['Ask'] - data['Bid']) * 10000\n",
        "\n",
        "    # Spread Analysis\n",
        "    print(\"\\nTick Spread Statistics (in BPS):\")\n",
        "    print(data['Spread_BPS'].describe())\n",
        "\n",
        "    # Plotting Price Evolution (Downsampled for performance)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    data['Price'].resample('1H').last().plot(\n",
        "        title='EUR/PLN Price Evolution (Hourly Resampled)',\n",
        "        ylabel='Price',\n",
        "        color='blue'\n",
        "    )\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    #\n",
        "\n",
        "    # Plotting Spread Distribution\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    data['Spread_BPS'].plot(\n",
        "        kind='hist',\n",
        "        bins=50,\n",
        "        title='Distribution of Tick Spreads (in BPS)',\n",
        "        xlabel='Spread (BPS)',\n",
        "        color='red',\n",
        "        alpha=0.7\n",
        "    )\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    #\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# SECTION 2: GARCH STRATEGY IMPLEMENTATION AND BACKTEST\n",
        "# ====================================================================\n",
        "\n",
        "def run_garch_strategy(data_full):\n",
        "    \"\"\"\n",
        "    Implements the GARCH-Adaptive Volatility Breakout strategy (Lecture 3).\n",
        "    \"\"\"\n",
        "    print(\"\\n--- 2.1 Starting GARCH Strategy Backtest ---\")\n",
        "\n",
        "    # 1. Calculate Log Returns\n",
        "    data_full['Log_Return'] = np.log(data_full['Price'] / data_full['Price'].shift(1))\n",
        "    data_full.dropna(subset=['Log_Return'], inplace=True)\n",
        "\n",
        "    # 2. Split Data Chronologically\n",
        "    train_data = data_full.loc[data_full.index < SPLIT_DATE].copy()\n",
        "    test_data = data_full.loc[(data_full.index >= SPLIT_DATE) & (data_full.index < END_PERIOD)].copy()\n",
        "\n",
        "    print(f\"\\nTraining Ticks (In-Sample): {len(train_data)}\")\n",
        "    print(f\"Testing Ticks (Out-of-Sample): {len(test_data)}\")\n",
        "\n",
        "    if test_data.empty:\n",
        "        print(\"ERROR: Testing data is empty. Check split dates and input file range.\")\n",
        "        return\n",
        "\n",
        "    # 3. GARCH MODEL FITTING (Core Statistical Tool - Lecture 3)\n",
        "    # Fit GARCH(1,1) on 1-minute bars for computational stability\n",
        "    print(\"Fitting GARCH(1,1) Model on 1-Minute resampled training data...\")\n",
        "    train_resampled = train_data['Price'].resample('1T').last().dropna()\n",
        "    train_resampled_returns = np.log(train_resampled / train_resampled.shift(1)).dropna()\n",
        "\n",
        "    # Scale returns by 1000 for better optimization\n",
        "    model = arch_model(train_resampled_returns * 1000, vol='Garch', p=1, q=1, mean='Zero', dist='Normal')\n",
        "    res = model.fit(disp='off')\n",
        "\n",
        "    # 4. VOLATILITY CALCULATION (Adaptive Band)\n",
        "    # Use Rolling Volatility on the Ticks as the adaptive band based on observed high-frequency noise.\n",
        "    # WINDOW_SIZE_TICKS = 15000 (approx. 1 hour of trading data)\n",
        "    WINDOW_SIZE_TICKS = 15000\n",
        "\n",
        "    # Apply rolling volatility to the entire dataset (including training data for lookback)\n",
        "    data_full['Rolling_Vol'] = data_full['Log_Return'].rolling(window=WINDOW_SIZE_TICKS).std()\n",
        "\n",
        "    # 5. SIGNAL GENERATION (Tick-by-Tick Breakout)\n",
        "    test_data['Rolling_Vol'] = data_full['Rolling_Vol'].loc[test_data.index]\n",
        "    test_data.dropna(subset=['Rolling_Vol'], inplace=True)\n",
        "\n",
        "    # Adaptive Breakout Bands\n",
        "    upper_band = test_data['Rolling_Vol'] * K_FACTOR\n",
        "    lower_band = -test_data['Rolling_Vol'] * K_FACTOR\n",
        "\n",
        "    test_data['Signal'] = 0\n",
        "    # Entry: Go Long (1) if the return breaks above the adaptive upper band\n",
        "    test_data.loc[test_data['Log_Return'] > upper_band, 'Signal'] = 1\n",
        "    # Entry: Go Short (-1) if the return breaks below the adaptive lower band\n",
        "    test_data.loc[test_data['Log_Return'] < lower_band, 'Signal'] = -1\n",
        "\n",
        "    # Position Management: Hold until a counter-signal is received\n",
        "    test_data['Position'] = test_data['Signal'].replace(0, np.nan).ffill().fillna(0)\n",
        "\n",
        "    # 6. P&L CALCULATION (Lecture 2)\n",
        "    test_data['Strategy_Ret_Gross'] = test_data['Position'].shift(1) * test_data['Log_Return']\n",
        "\n",
        "    # Net P&L: Gross PnL minus transaction costs\n",
        "    test_data['Turnover'] = test_data['Position'].diff().abs()\n",
        "    test_data['Transaction_Cost'] = test_data['Turnover'] * COST_BPS\n",
        "    test_data['Strategy_Ret_Net'] = test_data['Strategy_Ret_Gross'] - test_data['Transaction_Cost']\n",
        "\n",
        "    # Cumulative P&L\n",
        "    test_data['Cum_Gross_PnL'] = test_data['Strategy_Ret_Gross'].cumsum()\n",
        "    test_data['Cum_Net_PnL'] = test_data['Strategy_Ret_Net'].cumsum()\n",
        "\n",
        "    test_data.dropna(subset=['Cum_Net_PnL'], inplace=True)\n",
        "\n",
        "    # 7. PERFORMANCE METRICS (Lecture 2)\n",
        "    total_gross_return = test_data['Cum_Gross_PnL'].iloc[-1]\n",
        "    total_net_return = test_data['Cum_Net_PnL'].iloc[-1]\n",
        "\n",
        "    # Annualization Factor for Tick Data (252*24*3600 periods per year)\n",
        "    annualization_factor = np.sqrt(252 * 24 * 3600)\n",
        "\n",
        "    if test_data['Strategy_Ret_Net'].std() == 0:\n",
        "        sharpe_ratio = 0\n",
        "    else:\n",
        "        # Sharpe Ratio\n",
        "        sharpe_ratio = (test_data['Strategy_Ret_Net'].mean() / test_data['Strategy_Ret_Net'].std()) * annualization_factor\n",
        "\n",
        "    # Max Drawdown\n",
        "    cum_ret = (1 + test_data['Strategy_Ret_Net']).cumprod()\n",
        "    peak = cum_ret.cummax()\n",
        "    drawdown = (cum_ret - peak) / peak\n",
        "    max_drawdown = drawdown.min() if not drawdown.empty else 0\n",
        "\n",
        "    # 8. Output Metrics\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"   STRATEGY PERFORMANCE (OUT-OF-SAMPLE)\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Total Gross P&L:     {total_gross_return:.4f}\")\n",
        "    print(f\"Total Net P&L (Profit):{total_net_return:.4f}\")\n",
        "    print(f\"Annualized Sharpe Ratio:{sharpe_ratio:.4f}\")\n",
        "    print(f\"Max Drawdown:        {max_drawdown:.2%}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # 9. Plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(test_data.index, test_data['Cum_Net_PnL'], label='Net P&L (Tick-by-Tick)', color='#10B981')\n",
        "    plt.title('GARCH-Adaptive Volatility Breakout: Out-of-Sample Net P&L')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Cumulative Log Return')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    #\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# MAIN EXECUTION FLOW\n",
        "# ====================================================================\n",
        "if __name__ == '__main__':\n",
        "    # 1. Data Preparation and Loading\n",
        "    try:\n",
        "        clean_tick_data = load_and_combine_data()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nFATAL ERROR: Could not prepare data. {e}\")\n",
        "        clean_tick_data = pd.DataFrame()\n",
        "\n",
        "    if clean_tick_data.empty:\n",
        "        print(\"Execution halted due to data errors.\")\n",
        "    else:\n",
        "        # 2. Exploratory Data Analysis\n",
        "        run_eda(clean_tick_data)\n",
        "\n",
        "        # 3. Strategy Backtest\n",
        "        run_garch_strategy(clean_tick_data)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arch\n",
            "  Downloading arch-8.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from arch) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from arch) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.12/dist-packages (from arch) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from arch) (0.14.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from arch) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->arch) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->arch) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.4.0->arch) (2025.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.13.0->arch) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->arch) (1.17.0)\n",
            "Downloading arch-8.0.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.3/981.3 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: arch\n",
            "Successfully installed arch-8.0.0\n",
            "--- 1.1 Starting Data Loading and Combination ---\n",
            "Loading EURPLN-2025-09.csv assuming no header (Time, Bid, Ask, ...)\n",
            "Loaded 2811673 ticks from EURPLN-2025-09.csv\n",
            "Loading EURPLN-2025-10.csv assuming no header (Time, Bid, Ask, ...)\n",
            "Loaded 2583078 ticks from EURPLN-2025-10.csv\n",
            "Total combined ticks before filtering: 5394751\n",
            "\n",
            "FATAL ERROR: Could not prepare data. can only concatenate str (not \"float\") to str\n",
            "Execution halted due to data errors.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YEYBrti0mun",
        "outputId": "8d3ddcf5-cb0e-4e15-c877-6c6f6494e13e"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}